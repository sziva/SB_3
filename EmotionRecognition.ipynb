{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "EmotionRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQpjWg4Ld6m"
      },
      "source": [
        "## 1.  Connect google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyTEuhg7fR9P",
        "outputId": "3954a7c6-a3c8-431b-c3d3-4aa9906ded09"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQy7KUb6NBs-"
      },
      "source": [
        "## 2. Import Libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRPQRtXGmm2W"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential #Initialise our neural network model as a sequential network\n",
        "from keras.layers import Conv2D #Convolution operation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation#Applies activation function\n",
        "from keras.layers import Dropout#Prevents overfitting by randomly converting few outputs to zero\n",
        "from keras.layers import MaxPooling2D # Maxpooling function\n",
        "from keras.layers import Flatten # Converting 2D arrays into a 1D linear vector\n",
        "from keras.layers import Dense # Regular fully connected neural network\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from keras import backend\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAjYytuPDaU"
      },
      "source": [
        "## 3. Define Data loading mechanism.\n",
        "     \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2XHQzzzrRyF"
      },
      "source": [
        "def load_data(dataset_path):\n",
        "  \n",
        "  #classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprsie', 'Neutral']  #We will be dealing with seven different types of emotions.\n",
        "\n",
        "  data = []\n",
        "  test_data = []\n",
        "  test_labels = []\n",
        "  labels =[]\n",
        "\n",
        "  with open(dataset_path, 'r') as file:\n",
        "      for line_no, line in enumerate(file.readlines()):\n",
        "          if 0 < line_no <= 35887:\n",
        "            curr_class, line, set_type = line.split(',')\n",
        "            image_data = np.asarray([int(x) for x in line.split()]).reshape(48, 48)#Creating a list out of the string then converting it into a 2-Dimensional numpy array.\n",
        "            image_data =image_data.astype(np.uint8)/255.0\n",
        "            \n",
        "            if (set_type.strip() == 'PrivateTest'):\n",
        "              \n",
        "              test_data.append(image_data)\n",
        "              test_labels.append(curr_class)\n",
        "            else:\n",
        "              data.append(image_data)\n",
        "              labels.append(curr_class)\n",
        "      \n",
        "      test_data = np.expand_dims(test_data, -1)\n",
        "      test_labels = to_categorical(test_labels, num_classes = 7)\n",
        "      data = np.expand_dims(data, -1)   \n",
        "      labels = to_categorical(labels, num_classes = 7)\n",
        "    \n",
        "      return np.array(data), np.array(labels), np.array(test_data), np.array(test_labels)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFFZrnOgWOII"
      },
      "source": [
        "## 4. Load the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpdOghEhxmsd",
        "outputId": "41ee8748-7eef-414c-fb95-092bbbfb8556"
      },
      "source": [
        "dataset_path = \"/content/gdrive/My Drive/Colab Notebooks/fer2013.csv\" \n",
        "train_data, train_labels, test_data, test_labels = load_data(dataset_path)\n",
        "#train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = test_size,random_state = seed)\n",
        "\n",
        "#print(test_labels)\n",
        "#print(test_data)\n",
        "print(\"Number of images in Training set:\", len(train_data))\n",
        "print(\"Number of images in Test set:\", len(test_data))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Training set: 32298\n",
            "Number of images in Test set: 3589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rne3AL0_WnF0"
      },
      "source": [
        "## 5. Define and Deploy the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F3VAm1A-0fpD",
        "outputId": "c4617487-051e-4f40-e843-5f4b537de7e0"
      },
      "source": [
        "#######HYPERPARAMATERS###########\n",
        "epochs = 500\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "#################################\n",
        "\n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "adam = optimizers.Adam(lr = learning_rate)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "print(model.summary())\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=500, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "          train_data,\n",
        "          train_labels,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.2,\n",
        "          shuffle = True,\n",
        "          callbacks=[lr_reducer, checkpointer, early_stopper]\n",
        "          )\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_44 (Conv2D)           (None, 46, 46, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 46, 46, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 46, 46, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 23, 23, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 23, 23, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 23, 23, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 11, 11, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 11, 11, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 5, 5, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 5, 5, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 5, 5, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 7)                 455       \n",
            "=================================================================\n",
            "Total params: 9,014,727\n",
            "Trainable params: 9,009,223\n",
            "Non-trainable params: 5,504\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/500\n",
            "404/404 [==============================] - 27s 64ms/step - loss: 2.6459 - accuracy: 0.1820 - val_loss: 1.8814 - val_accuracy: 0.2489\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.88144, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 2/500\n",
            "404/404 [==============================] - 26s 64ms/step - loss: 1.8654 - accuracy: 0.2293 - val_loss: 1.8318 - val_accuracy: 0.2489\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.88144 to 1.83179, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 3/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.8429 - accuracy: 0.2471 - val_loss: 1.8303 - val_accuracy: 0.2489\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.83179 to 1.83031, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 4/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.8355 - accuracy: 0.2498 - val_loss: 1.8163 - val_accuracy: 0.2489\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.83031 to 1.81628, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 5/500\n",
            "404/404 [==============================] - 26s 63ms/step - loss: 1.8246 - accuracy: 0.2482 - val_loss: 1.8135 - val_accuracy: 0.2489\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.81628 to 1.81352, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 6/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.8191 - accuracy: 0.2501 - val_loss: 1.8132 - val_accuracy: 0.2489\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.81352 to 1.81324, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 7/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.8133 - accuracy: 0.2543 - val_loss: 1.7909 - val_accuracy: 0.2488\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.81324 to 1.79089, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 8/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.7943 - accuracy: 0.2598 - val_loss: 1.8505 - val_accuracy: 0.1788\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.79089\n",
            "Epoch 9/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.7540 - accuracy: 0.2860 - val_loss: 1.7264 - val_accuracy: 0.2872\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.79089 to 1.72644, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 10/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.7053 - accuracy: 0.3064 - val_loss: 1.8450 - val_accuracy: 0.2687\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.72644\n",
            "Epoch 11/500\n",
            "404/404 [==============================] - 26s 63ms/step - loss: 1.6475 - accuracy: 0.3226 - val_loss: 1.5182 - val_accuracy: 0.3579\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.72644 to 1.51816, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 12/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.5860 - accuracy: 0.3611 - val_loss: 1.5819 - val_accuracy: 0.3604\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.51816\n",
            "Epoch 13/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.5426 - accuracy: 0.3937 - val_loss: 1.5665 - val_accuracy: 0.3658\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.51816\n",
            "Epoch 14/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.5136 - accuracy: 0.4052 - val_loss: 1.4564 - val_accuracy: 0.4088\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.51816 to 1.45636, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 15/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.4922 - accuracy: 0.4079 - val_loss: 1.5149 - val_accuracy: 0.3890\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.45636\n",
            "Epoch 16/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.4648 - accuracy: 0.4104 - val_loss: 1.4949 - val_accuracy: 0.3895\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.45636\n",
            "Epoch 17/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.4462 - accuracy: 0.4298 - val_loss: 1.4678 - val_accuracy: 0.3837\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.45636\n",
            "Epoch 18/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.4237 - accuracy: 0.4316 - val_loss: 1.4998 - val_accuracy: 0.3865\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.45636\n",
            "Epoch 19/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.4050 - accuracy: 0.4316 - val_loss: 1.4410 - val_accuracy: 0.4071\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.45636 to 1.44099, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 20/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.4029 - accuracy: 0.4392 - val_loss: 1.3492 - val_accuracy: 0.4536\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.44099 to 1.34915, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 21/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 1.3858 - accuracy: 0.4480 - val_loss: 1.3899 - val_accuracy: 0.4424\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.34915\n",
            "Epoch 22/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.3531 - accuracy: 0.4546 - val_loss: 1.3598 - val_accuracy: 0.4378\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.34915\n",
            "Epoch 23/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.3390 - accuracy: 0.4555 - val_loss: 1.3828 - val_accuracy: 0.4368\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.34915\n",
            "Epoch 24/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.3395 - accuracy: 0.4581 - val_loss: 1.3996 - val_accuracy: 0.4494\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.34915\n",
            "Epoch 25/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.3356 - accuracy: 0.4595 - val_loss: 1.3710 - val_accuracy: 0.4560\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.34915\n",
            "Epoch 26/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.3132 - accuracy: 0.4576 - val_loss: 1.5209 - val_accuracy: 0.3950\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.34915\n",
            "Epoch 27/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.2989 - accuracy: 0.4710 - val_loss: 1.2679 - val_accuracy: 0.4848\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.34915 to 1.26791, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 28/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.2799 - accuracy: 0.4822 - val_loss: 1.2707 - val_accuracy: 0.4844\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.26791\n",
            "Epoch 29/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.2564 - accuracy: 0.4849 - val_loss: 1.2997 - val_accuracy: 0.4724\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.26791\n",
            "Epoch 30/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.2618 - accuracy: 0.4874 - val_loss: 1.2587 - val_accuracy: 0.5019\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.26791 to 1.25874, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 31/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.2275 - accuracy: 0.5041 - val_loss: 1.2325 - val_accuracy: 0.5201\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.25874 to 1.23252, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 32/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.2205 - accuracy: 0.5160 - val_loss: 1.2200 - val_accuracy: 0.5364\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.23252 to 1.22000, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 33/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.2068 - accuracy: 0.5224 - val_loss: 1.2124 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.22000 to 1.21236, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 34/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.1862 - accuracy: 0.5311 - val_loss: 1.2214 - val_accuracy: 0.5195\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.21236\n",
            "Epoch 35/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.1587 - accuracy: 0.5460 - val_loss: 1.2530 - val_accuracy: 0.5020\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.21236\n",
            "Epoch 36/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.1596 - accuracy: 0.5448 - val_loss: 1.1897 - val_accuracy: 0.5480\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.21236 to 1.18971, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 37/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.1245 - accuracy: 0.5610 - val_loss: 1.2260 - val_accuracy: 0.5280\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.18971\n",
            "Epoch 38/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.1063 - accuracy: 0.5706 - val_loss: 1.2007 - val_accuracy: 0.5441\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.18971\n",
            "Epoch 39/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.1109 - accuracy: 0.5771 - val_loss: 1.2379 - val_accuracy: 0.5279\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.18971\n",
            "Epoch 40/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.0961 - accuracy: 0.5817 - val_loss: 1.2044 - val_accuracy: 0.5437\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.18971\n",
            "Epoch 41/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.0664 - accuracy: 0.5937 - val_loss: 1.2315 - val_accuracy: 0.5483\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.18971\n",
            "Epoch 42/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.0528 - accuracy: 0.5942 - val_loss: 1.2095 - val_accuracy: 0.5534\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.18971\n",
            "Epoch 43/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.0202 - accuracy: 0.6135 - val_loss: 1.2534 - val_accuracy: 0.5449\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.18971\n",
            "Epoch 44/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 1.0107 - accuracy: 0.6179 - val_loss: 1.2147 - val_accuracy: 0.5670\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.18971\n",
            "Epoch 45/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.9993 - accuracy: 0.6189 - val_loss: 1.1942 - val_accuracy: 0.5680\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.18971\n",
            "Epoch 46/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.9971 - accuracy: 0.6262 - val_loss: 1.2401 - val_accuracy: 0.5522\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.18971\n",
            "Epoch 47/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.9693 - accuracy: 0.6433 - val_loss: 1.1845 - val_accuracy: 0.5893\n",
            "\n",
            "Epoch 00047: val_loss improved from 1.18971 to 1.18448, saving model to /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Colab Notebooks/Model500/weights.hd5/assets\n",
            "Epoch 48/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.9502 - accuracy: 0.6464 - val_loss: 1.2114 - val_accuracy: 0.5760\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.18448\n",
            "Epoch 49/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.9186 - accuracy: 0.6589 - val_loss: 1.2039 - val_accuracy: 0.5800\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.18448\n",
            "Epoch 50/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.9407 - accuracy: 0.6507 - val_loss: 1.1980 - val_accuracy: 0.5918\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.18448\n",
            "Epoch 51/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.9000 - accuracy: 0.6683 - val_loss: 1.2076 - val_accuracy: 0.5986\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.18448\n",
            "Epoch 52/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.8731 - accuracy: 0.6834 - val_loss: 1.2267 - val_accuracy: 0.5895\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.18448\n",
            "Epoch 53/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.8689 - accuracy: 0.6777 - val_loss: 1.2192 - val_accuracy: 0.5884\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.18448\n",
            "Epoch 54/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.8640 - accuracy: 0.6853 - val_loss: 1.2705 - val_accuracy: 0.5947\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.18448\n",
            "Epoch 55/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.8349 - accuracy: 0.6984 - val_loss: 1.2824 - val_accuracy: 0.6036\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.18448\n",
            "Epoch 56/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.8278 - accuracy: 0.7034 - val_loss: 1.2050 - val_accuracy: 0.5932\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.18448\n",
            "Epoch 57/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.8130 - accuracy: 0.7110 - val_loss: 1.3187 - val_accuracy: 0.5940\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.18448\n",
            "Epoch 58/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.7936 - accuracy: 0.7166 - val_loss: 1.3136 - val_accuracy: 0.6087\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.18448\n",
            "Epoch 59/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.7825 - accuracy: 0.7204 - val_loss: 1.2899 - val_accuracy: 0.6060\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.18448\n",
            "Epoch 60/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.7584 - accuracy: 0.7381 - val_loss: 1.3328 - val_accuracy: 0.6144\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.18448\n",
            "Epoch 61/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.7618 - accuracy: 0.7346 - val_loss: 1.2957 - val_accuracy: 0.6036\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.18448\n",
            "Epoch 62/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.7401 - accuracy: 0.7392 - val_loss: 1.3450 - val_accuracy: 0.6121\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.18448\n",
            "Epoch 63/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.7095 - accuracy: 0.7510 - val_loss: 1.3309 - val_accuracy: 0.6139\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.18448\n",
            "Epoch 64/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.7150 - accuracy: 0.7477 - val_loss: 1.2957 - val_accuracy: 0.6042\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.18448\n",
            "Epoch 65/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.6898 - accuracy: 0.7588 - val_loss: 1.3313 - val_accuracy: 0.6017\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.18448\n",
            "Epoch 66/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.6806 - accuracy: 0.7635 - val_loss: 1.3618 - val_accuracy: 0.6186\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.18448\n",
            "Epoch 67/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.6776 - accuracy: 0.7691 - val_loss: 1.3914 - val_accuracy: 0.6119\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.18448\n",
            "Epoch 68/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.6489 - accuracy: 0.7790 - val_loss: 1.4639 - val_accuracy: 0.6110\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.18448\n",
            "Epoch 69/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.6447 - accuracy: 0.7816 - val_loss: 1.5057 - val_accuracy: 0.6231\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.18448\n",
            "Epoch 70/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.6273 - accuracy: 0.7901 - val_loss: 1.4153 - val_accuracy: 0.6226\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.18448\n",
            "Epoch 71/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.6251 - accuracy: 0.7887 - val_loss: 1.4682 - val_accuracy: 0.6184\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.18448\n",
            "Epoch 72/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.6182 - accuracy: 0.7922 - val_loss: 1.4743 - val_accuracy: 0.6215\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.18448\n",
            "Epoch 73/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5927 - accuracy: 0.7998 - val_loss: 1.4846 - val_accuracy: 0.6190\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.18448\n",
            "Epoch 74/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5840 - accuracy: 0.8083 - val_loss: 1.5522 - val_accuracy: 0.6246\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.18448\n",
            "Epoch 75/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5767 - accuracy: 0.8085 - val_loss: 1.5591 - val_accuracy: 0.6245\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.18448\n",
            "Epoch 76/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5711 - accuracy: 0.8092 - val_loss: 1.5162 - val_accuracy: 0.6228\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.18448\n",
            "Epoch 77/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5539 - accuracy: 0.8181 - val_loss: 1.5827 - val_accuracy: 0.6272\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.18448\n",
            "Epoch 78/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5458 - accuracy: 0.8203 - val_loss: 1.6057 - val_accuracy: 0.6175\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.18448\n",
            "Epoch 79/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5355 - accuracy: 0.8251 - val_loss: 1.6761 - val_accuracy: 0.6229\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.18448\n",
            "Epoch 80/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5309 - accuracy: 0.8267 - val_loss: 1.6693 - val_accuracy: 0.6238\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.18448\n",
            "Epoch 81/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5233 - accuracy: 0.8328 - val_loss: 1.6480 - val_accuracy: 0.6248\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.18448\n",
            "Epoch 82/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5133 - accuracy: 0.8348 - val_loss: 1.6770 - val_accuracy: 0.6280\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.18448\n",
            "Epoch 83/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5128 - accuracy: 0.8337 - val_loss: 1.7962 - val_accuracy: 0.6254\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.18448\n",
            "Epoch 84/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5055 - accuracy: 0.8421 - val_loss: 1.6522 - val_accuracy: 0.6243\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.18448\n",
            "Epoch 85/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.5018 - accuracy: 0.8417 - val_loss: 1.6630 - val_accuracy: 0.6234\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.18448\n",
            "Epoch 86/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4871 - accuracy: 0.8405 - val_loss: 1.8550 - val_accuracy: 0.6147\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.18448\n",
            "Epoch 87/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4899 - accuracy: 0.8451 - val_loss: 1.7775 - val_accuracy: 0.6192\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.18448\n",
            "Epoch 88/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4737 - accuracy: 0.8438 - val_loss: 1.7242 - val_accuracy: 0.6282\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.18448\n",
            "Epoch 89/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4613 - accuracy: 0.8517 - val_loss: 1.7372 - val_accuracy: 0.6248\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.18448\n",
            "Epoch 90/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4711 - accuracy: 0.8499 - val_loss: 1.7110 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.18448\n",
            "Epoch 91/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4432 - accuracy: 0.8592 - val_loss: 1.8010 - val_accuracy: 0.6303\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.18448\n",
            "Epoch 92/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4470 - accuracy: 0.8586 - val_loss: 1.8198 - val_accuracy: 0.6248\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.18448\n",
            "Epoch 93/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4474 - accuracy: 0.8580 - val_loss: 1.8136 - val_accuracy: 0.6257\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.18448\n",
            "Epoch 94/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4247 - accuracy: 0.8688 - val_loss: 1.8435 - val_accuracy: 0.6280\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.18448\n",
            "Epoch 95/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4329 - accuracy: 0.8648 - val_loss: 1.8704 - val_accuracy: 0.6316\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.18448\n",
            "Epoch 96/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4324 - accuracy: 0.8643 - val_loss: 1.9612 - val_accuracy: 0.6325\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.18448\n",
            "Epoch 97/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.4321 - accuracy: 0.8636 - val_loss: 1.8930 - val_accuracy: 0.6353\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.18448\n",
            "Epoch 98/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4078 - accuracy: 0.8734 - val_loss: 1.9181 - val_accuracy: 0.6313\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.18448\n",
            "Epoch 99/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4063 - accuracy: 0.8701 - val_loss: 1.9220 - val_accuracy: 0.6320\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.18448\n",
            "Epoch 100/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4110 - accuracy: 0.8721 - val_loss: 1.9346 - val_accuracy: 0.6347\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.18448\n",
            "Epoch 101/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4065 - accuracy: 0.8745 - val_loss: 1.9141 - val_accuracy: 0.6325\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.18448\n",
            "Epoch 102/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4070 - accuracy: 0.8744 - val_loss: 2.0046 - val_accuracy: 0.6291\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.18448\n",
            "Epoch 103/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3995 - accuracy: 0.8767 - val_loss: 2.0244 - val_accuracy: 0.6322\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.18448\n",
            "Epoch 104/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3951 - accuracy: 0.8764 - val_loss: 2.0369 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.18448\n",
            "Epoch 105/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.4015 - accuracy: 0.8760 - val_loss: 1.8955 - val_accuracy: 0.6364\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.18448\n",
            "Epoch 106/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3915 - accuracy: 0.8759 - val_loss: 1.9602 - val_accuracy: 0.6339\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.18448\n",
            "Epoch 107/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3777 - accuracy: 0.8844 - val_loss: 2.0775 - val_accuracy: 0.6350\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.18448\n",
            "Epoch 108/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3790 - accuracy: 0.8838 - val_loss: 1.9506 - val_accuracy: 0.6294\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.18448\n",
            "Epoch 109/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.3830 - accuracy: 0.8814 - val_loss: 2.0032 - val_accuracy: 0.6334\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.18448\n",
            "Epoch 110/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3783 - accuracy: 0.8844 - val_loss: 2.0417 - val_accuracy: 0.6339\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.18448\n",
            "Epoch 111/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3662 - accuracy: 0.8838 - val_loss: 2.0260 - val_accuracy: 0.6311\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.18448\n",
            "Epoch 112/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3667 - accuracy: 0.8846 - val_loss: 2.0415 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.18448\n",
            "Epoch 113/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3704 - accuracy: 0.8881 - val_loss: 2.0551 - val_accuracy: 0.6362\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.18448\n",
            "Epoch 114/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3650 - accuracy: 0.8885 - val_loss: 2.1073 - val_accuracy: 0.6345\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.18448\n",
            "Epoch 115/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3702 - accuracy: 0.8866 - val_loss: 2.1111 - val_accuracy: 0.6339\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.18448\n",
            "Epoch 116/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3461 - accuracy: 0.8946 - val_loss: 2.0788 - val_accuracy: 0.6327\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.18448\n",
            "Epoch 117/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3513 - accuracy: 0.8912 - val_loss: 2.0919 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.18448\n",
            "Epoch 118/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3694 - accuracy: 0.8881 - val_loss: 2.0687 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.18448\n",
            "Epoch 119/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3562 - accuracy: 0.8895 - val_loss: 2.1075 - val_accuracy: 0.6356\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.18448\n",
            "Epoch 120/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3420 - accuracy: 0.8954 - val_loss: 2.0790 - val_accuracy: 0.6347\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.18448\n",
            "Epoch 121/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3392 - accuracy: 0.8967 - val_loss: 2.1752 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.18448\n",
            "Epoch 122/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.3440 - accuracy: 0.8950 - val_loss: 2.1488 - val_accuracy: 0.6362\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.18448\n",
            "Epoch 123/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3520 - accuracy: 0.8893 - val_loss: 2.1480 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.18448\n",
            "Epoch 124/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3479 - accuracy: 0.8931 - val_loss: 2.1535 - val_accuracy: 0.6432\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.18448\n",
            "Epoch 125/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3368 - accuracy: 0.8978 - val_loss: 2.1643 - val_accuracy: 0.6393\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.18448\n",
            "Epoch 126/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3394 - accuracy: 0.8957 - val_loss: 2.1454 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.18448\n",
            "Epoch 127/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3388 - accuracy: 0.8975 - val_loss: 2.1566 - val_accuracy: 0.6392\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.18448\n",
            "Epoch 128/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3355 - accuracy: 0.8989 - val_loss: 2.1984 - val_accuracy: 0.6396\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.18448\n",
            "Epoch 129/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3203 - accuracy: 0.9032 - val_loss: 2.1726 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.18448\n",
            "Epoch 130/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3292 - accuracy: 0.9003 - val_loss: 2.2324 - val_accuracy: 0.6387\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.18448\n",
            "Epoch 131/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3283 - accuracy: 0.9028 - val_loss: 2.1691 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.18448\n",
            "Epoch 132/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3280 - accuracy: 0.8987 - val_loss: 2.1902 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.18448\n",
            "Epoch 133/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3347 - accuracy: 0.9000 - val_loss: 2.2162 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.18448\n",
            "Epoch 134/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.3463 - accuracy: 0.8950 - val_loss: 2.2371 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.18448\n",
            "Epoch 135/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.3206 - accuracy: 0.8998 - val_loss: 2.2588 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.18448\n",
            "Epoch 136/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3163 - accuracy: 0.9053 - val_loss: 2.2985 - val_accuracy: 0.6344\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.18448\n",
            "Epoch 137/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3165 - accuracy: 0.9039 - val_loss: 2.2442 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.18448\n",
            "Epoch 138/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3054 - accuracy: 0.9049 - val_loss: 2.2491 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.18448\n",
            "Epoch 139/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3113 - accuracy: 0.9045 - val_loss: 2.2976 - val_accuracy: 0.6356\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.18448\n",
            "Epoch 140/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3194 - accuracy: 0.9018 - val_loss: 2.2700 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.18448\n",
            "Epoch 141/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3169 - accuracy: 0.9061 - val_loss: 2.2810 - val_accuracy: 0.6364\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.18448\n",
            "Epoch 142/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3160 - accuracy: 0.9034 - val_loss: 2.2635 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.18448\n",
            "Epoch 143/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3139 - accuracy: 0.9035 - val_loss: 2.2699 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.18448\n",
            "Epoch 144/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3145 - accuracy: 0.9037 - val_loss: 2.2639 - val_accuracy: 0.6356\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.18448\n",
            "Epoch 145/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3097 - accuracy: 0.9059 - val_loss: 2.2995 - val_accuracy: 0.6362\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.18448\n",
            "Epoch 146/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3057 - accuracy: 0.9071 - val_loss: 2.3256 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.18448\n",
            "Epoch 147/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3049 - accuracy: 0.9078 - val_loss: 2.3124 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.18448\n",
            "Epoch 148/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3036 - accuracy: 0.9073 - val_loss: 2.3317 - val_accuracy: 0.6361\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.18448\n",
            "Epoch 149/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3068 - accuracy: 0.9052 - val_loss: 2.3397 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.18448\n",
            "Epoch 150/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3059 - accuracy: 0.9077 - val_loss: 2.3325 - val_accuracy: 0.6359\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.18448\n",
            "Epoch 151/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3148 - accuracy: 0.9029 - val_loss: 2.3230 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.18448\n",
            "Epoch 152/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3109 - accuracy: 0.9059 - val_loss: 2.3292 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.18448\n",
            "Epoch 153/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3124 - accuracy: 0.9053 - val_loss: 2.3094 - val_accuracy: 0.6410\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.18448\n",
            "Epoch 154/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3063 - accuracy: 0.9091 - val_loss: 2.2875 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.18448\n",
            "Epoch 155/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3140 - accuracy: 0.9064 - val_loss: 2.3069 - val_accuracy: 0.6365\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.18448\n",
            "Epoch 156/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3128 - accuracy: 0.9049 - val_loss: 2.3122 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.18448\n",
            "Epoch 157/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3078 - accuracy: 0.9078 - val_loss: 2.2936 - val_accuracy: 0.6390\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.18448\n",
            "Epoch 158/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2986 - accuracy: 0.9065 - val_loss: 2.3150 - val_accuracy: 0.6365\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.18448\n",
            "Epoch 159/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3102 - accuracy: 0.9046 - val_loss: 2.3451 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.18448\n",
            "Epoch 160/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2946 - accuracy: 0.9083 - val_loss: 2.3154 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.18448\n",
            "Epoch 161/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3181 - accuracy: 0.9065 - val_loss: 2.2987 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.18448\n",
            "Epoch 162/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2957 - accuracy: 0.9086 - val_loss: 2.3134 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.18448\n",
            "Epoch 163/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2904 - accuracy: 0.9084 - val_loss: 2.3375 - val_accuracy: 0.6362\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.18448\n",
            "Epoch 164/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2957 - accuracy: 0.9118 - val_loss: 2.3372 - val_accuracy: 0.6387\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.18448\n",
            "Epoch 165/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2851 - accuracy: 0.9161 - val_loss: 2.3461 - val_accuracy: 0.6393\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.18448\n",
            "Epoch 166/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3019 - accuracy: 0.9104 - val_loss: 2.3452 - val_accuracy: 0.6390\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.18448\n",
            "Epoch 167/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3024 - accuracy: 0.9080 - val_loss: 2.3370 - val_accuracy: 0.6402\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.18448\n",
            "Epoch 168/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2962 - accuracy: 0.9094 - val_loss: 2.3489 - val_accuracy: 0.6402\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.18448\n",
            "Epoch 169/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2938 - accuracy: 0.9103 - val_loss: 2.3633 - val_accuracy: 0.6395\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.18448\n",
            "Epoch 170/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3037 - accuracy: 0.9112 - val_loss: 2.3711 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.18448\n",
            "Epoch 171/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3052 - accuracy: 0.9096 - val_loss: 2.3695 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.18448\n",
            "Epoch 172/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2908 - accuracy: 0.9094 - val_loss: 2.3528 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.18448\n",
            "Epoch 173/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3030 - accuracy: 0.9080 - val_loss: 2.3750 - val_accuracy: 0.6364\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.18448\n",
            "Epoch 174/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3026 - accuracy: 0.9068 - val_loss: 2.3733 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.18448\n",
            "Epoch 175/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2958 - accuracy: 0.9117 - val_loss: 2.3606 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.18448\n",
            "Epoch 176/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2988 - accuracy: 0.9122 - val_loss: 2.3632 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.18448\n",
            "Epoch 177/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2978 - accuracy: 0.9128 - val_loss: 2.3511 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.18448\n",
            "Epoch 178/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3117 - accuracy: 0.9092 - val_loss: 2.3770 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.18448\n",
            "Epoch 179/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2851 - accuracy: 0.9145 - val_loss: 2.3678 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.18448\n",
            "Epoch 180/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3018 - accuracy: 0.9081 - val_loss: 2.3723 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.18448\n",
            "Epoch 181/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2975 - accuracy: 0.9104 - val_loss: 2.3732 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.18448\n",
            "Epoch 182/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2967 - accuracy: 0.9108 - val_loss: 2.3494 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.18448\n",
            "Epoch 183/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3025 - accuracy: 0.9078 - val_loss: 2.3736 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.18448\n",
            "Epoch 184/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3074 - accuracy: 0.9065 - val_loss: 2.3479 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.18448\n",
            "Epoch 185/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2899 - accuracy: 0.9115 - val_loss: 2.3656 - val_accuracy: 0.6390\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.18448\n",
            "Epoch 186/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2974 - accuracy: 0.9097 - val_loss: 2.3595 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.18448\n",
            "Epoch 187/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2952 - accuracy: 0.9103 - val_loss: 2.3768 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.18448\n",
            "Epoch 188/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2904 - accuracy: 0.9140 - val_loss: 2.3701 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.18448\n",
            "Epoch 189/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2932 - accuracy: 0.9111 - val_loss: 2.3883 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.18448\n",
            "Epoch 190/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2807 - accuracy: 0.9129 - val_loss: 2.3919 - val_accuracy: 0.6392\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.18448\n",
            "Epoch 191/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2972 - accuracy: 0.9097 - val_loss: 2.3780 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.18448\n",
            "Epoch 192/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2906 - accuracy: 0.9113 - val_loss: 2.3852 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.18448\n",
            "Epoch 193/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2941 - accuracy: 0.9134 - val_loss: 2.3849 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.18448\n",
            "Epoch 194/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2966 - accuracy: 0.9093 - val_loss: 2.3819 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.18448\n",
            "Epoch 195/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2913 - accuracy: 0.9121 - val_loss: 2.3726 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.18448\n",
            "Epoch 196/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2885 - accuracy: 0.9121 - val_loss: 2.3962 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.18448\n",
            "Epoch 197/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2959 - accuracy: 0.9091 - val_loss: 2.3751 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.18448\n",
            "Epoch 198/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2832 - accuracy: 0.9130 - val_loss: 2.3860 - val_accuracy: 0.6387\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.18448\n",
            "Epoch 199/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2940 - accuracy: 0.9122 - val_loss: 2.3795 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.18448\n",
            "Epoch 200/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2786 - accuracy: 0.9151 - val_loss: 2.3815 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.18448\n",
            "Epoch 201/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2938 - accuracy: 0.9139 - val_loss: 2.3743 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.18448\n",
            "Epoch 202/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2944 - accuracy: 0.9113 - val_loss: 2.3782 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.18448\n",
            "Epoch 203/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2803 - accuracy: 0.9165 - val_loss: 2.3840 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.18448\n",
            "Epoch 204/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2892 - accuracy: 0.9138 - val_loss: 2.3845 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.18448\n",
            "Epoch 205/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2997 - accuracy: 0.9111 - val_loss: 2.3759 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.18448\n",
            "Epoch 206/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2932 - accuracy: 0.9125 - val_loss: 2.3795 - val_accuracy: 0.6390\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.18448\n",
            "Epoch 207/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2942 - accuracy: 0.9108 - val_loss: 2.3824 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.18448\n",
            "Epoch 208/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2793 - accuracy: 0.9141 - val_loss: 2.4059 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.18448\n",
            "Epoch 209/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2990 - accuracy: 0.9088 - val_loss: 2.3947 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.18448\n",
            "Epoch 210/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2837 - accuracy: 0.9150 - val_loss: 2.3985 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.18448\n",
            "Epoch 211/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2806 - accuracy: 0.9163 - val_loss: 2.3697 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.18448\n",
            "Epoch 212/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2983 - accuracy: 0.9094 - val_loss: 2.3940 - val_accuracy: 0.6395\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.18448\n",
            "Epoch 213/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2903 - accuracy: 0.9110 - val_loss: 2.3848 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.18448\n",
            "Epoch 214/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2923 - accuracy: 0.9121 - val_loss: 2.4025 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.18448\n",
            "Epoch 215/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2942 - accuracy: 0.9126 - val_loss: 2.3963 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.18448\n",
            "Epoch 216/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2947 - accuracy: 0.9122 - val_loss: 2.3979 - val_accuracy: 0.6392\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.18448\n",
            "Epoch 217/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2826 - accuracy: 0.9157 - val_loss: 2.4044 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.18448\n",
            "Epoch 218/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2913 - accuracy: 0.9080 - val_loss: 2.4024 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.18448\n",
            "Epoch 219/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3058 - accuracy: 0.9103 - val_loss: 2.3942 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.18448\n",
            "Epoch 220/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2926 - accuracy: 0.9140 - val_loss: 2.3909 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.18448\n",
            "Epoch 221/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3046 - accuracy: 0.9107 - val_loss: 2.3982 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.18448\n",
            "Epoch 222/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2934 - accuracy: 0.9133 - val_loss: 2.4010 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.18448\n",
            "Epoch 223/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2954 - accuracy: 0.9102 - val_loss: 2.4036 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.18448\n",
            "Epoch 224/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2862 - accuracy: 0.9148 - val_loss: 2.3897 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.18448\n",
            "Epoch 225/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2866 - accuracy: 0.9129 - val_loss: 2.3786 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.18448\n",
            "Epoch 226/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2962 - accuracy: 0.9084 - val_loss: 2.4077 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.18448\n",
            "Epoch 227/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2959 - accuracy: 0.9124 - val_loss: 2.3993 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.18448\n",
            "Epoch 228/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2852 - accuracy: 0.9146 - val_loss: 2.3854 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.18448\n",
            "Epoch 229/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2899 - accuracy: 0.9153 - val_loss: 2.4094 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.18448\n",
            "Epoch 230/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3028 - accuracy: 0.9104 - val_loss: 2.4117 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.18448\n",
            "Epoch 231/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2951 - accuracy: 0.9112 - val_loss: 2.3956 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.18448\n",
            "Epoch 232/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2952 - accuracy: 0.9080 - val_loss: 2.4091 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.18448\n",
            "Epoch 233/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2901 - accuracy: 0.9116 - val_loss: 2.4053 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.18448\n",
            "Epoch 234/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2843 - accuracy: 0.9121 - val_loss: 2.3979 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.18448\n",
            "Epoch 235/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2942 - accuracy: 0.9130 - val_loss: 2.4062 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.18448\n",
            "Epoch 236/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2890 - accuracy: 0.9140 - val_loss: 2.4090 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.18448\n",
            "Epoch 237/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2885 - accuracy: 0.9123 - val_loss: 2.4085 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.18448\n",
            "Epoch 238/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2904 - accuracy: 0.9139 - val_loss: 2.4059 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.18448\n",
            "Epoch 239/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2945 - accuracy: 0.9113 - val_loss: 2.4035 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.18448\n",
            "Epoch 240/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2855 - accuracy: 0.9131 - val_loss: 2.4126 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.18448\n",
            "Epoch 241/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2800 - accuracy: 0.9156 - val_loss: 2.4050 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.18448\n",
            "Epoch 242/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2961 - accuracy: 0.9134 - val_loss: 2.3938 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.18448\n",
            "Epoch 243/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2888 - accuracy: 0.9124 - val_loss: 2.4022 - val_accuracy: 0.6390\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.18448\n",
            "Epoch 244/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2882 - accuracy: 0.9137 - val_loss: 2.3937 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.18448\n",
            "Epoch 245/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2848 - accuracy: 0.9131 - val_loss: 2.3968 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.18448\n",
            "Epoch 246/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2988 - accuracy: 0.9127 - val_loss: 2.4048 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.18448\n",
            "Epoch 247/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2889 - accuracy: 0.9143 - val_loss: 2.4012 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.18448\n",
            "Epoch 248/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2923 - accuracy: 0.9107 - val_loss: 2.3942 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.18448\n",
            "Epoch 249/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2933 - accuracy: 0.9110 - val_loss: 2.3906 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.18448\n",
            "Epoch 250/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2881 - accuracy: 0.9140 - val_loss: 2.3982 - val_accuracy: 0.6387\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.18448\n",
            "Epoch 251/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2944 - accuracy: 0.9119 - val_loss: 2.3957 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.18448\n",
            "Epoch 252/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2853 - accuracy: 0.9148 - val_loss: 2.3982 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.18448\n",
            "Epoch 253/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2849 - accuracy: 0.9173 - val_loss: 2.4179 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.18448\n",
            "Epoch 254/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2958 - accuracy: 0.9125 - val_loss: 2.3952 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.18448\n",
            "Epoch 255/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2848 - accuracy: 0.9133 - val_loss: 2.4021 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.18448\n",
            "Epoch 256/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2840 - accuracy: 0.9120 - val_loss: 2.4046 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.18448\n",
            "Epoch 257/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2877 - accuracy: 0.9137 - val_loss: 2.4089 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.18448\n",
            "Epoch 258/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2938 - accuracy: 0.9139 - val_loss: 2.4142 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.18448\n",
            "Epoch 259/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2804 - accuracy: 0.9158 - val_loss: 2.4164 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.18448\n",
            "Epoch 260/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2865 - accuracy: 0.9140 - val_loss: 2.3974 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.18448\n",
            "Epoch 261/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2918 - accuracy: 0.9128 - val_loss: 2.4152 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.18448\n",
            "Epoch 262/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2866 - accuracy: 0.9127 - val_loss: 2.4024 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.18448\n",
            "Epoch 263/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2874 - accuracy: 0.9135 - val_loss: 2.3886 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.18448\n",
            "Epoch 264/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2882 - accuracy: 0.9123 - val_loss: 2.4106 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.18448\n",
            "Epoch 265/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3047 - accuracy: 0.9087 - val_loss: 2.4194 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.18448\n",
            "Epoch 266/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2964 - accuracy: 0.9110 - val_loss: 2.4181 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.18448\n",
            "Epoch 267/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2883 - accuracy: 0.9150 - val_loss: 2.3906 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.18448\n",
            "Epoch 268/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2872 - accuracy: 0.9115 - val_loss: 2.4080 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.18448\n",
            "Epoch 269/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2935 - accuracy: 0.9112 - val_loss: 2.4023 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.18448\n",
            "Epoch 270/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2798 - accuracy: 0.9146 - val_loss: 2.4162 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.18448\n",
            "Epoch 271/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2982 - accuracy: 0.9107 - val_loss: 2.3906 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.18448\n",
            "Epoch 272/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2964 - accuracy: 0.9128 - val_loss: 2.4122 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.18448\n",
            "Epoch 273/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2801 - accuracy: 0.9145 - val_loss: 2.4052 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.18448\n",
            "Epoch 274/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2872 - accuracy: 0.9125 - val_loss: 2.3958 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.18448\n",
            "Epoch 275/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2865 - accuracy: 0.9155 - val_loss: 2.4035 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.18448\n",
            "Epoch 276/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2875 - accuracy: 0.9147 - val_loss: 2.3982 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.18448\n",
            "Epoch 277/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2860 - accuracy: 0.9130 - val_loss: 2.4022 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.18448\n",
            "Epoch 278/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2840 - accuracy: 0.9153 - val_loss: 2.3944 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.18448\n",
            "Epoch 279/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2845 - accuracy: 0.9147 - val_loss: 2.4083 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.18448\n",
            "Epoch 280/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2977 - accuracy: 0.9115 - val_loss: 2.3944 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.18448\n",
            "Epoch 281/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2807 - accuracy: 0.9151 - val_loss: 2.4011 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.18448\n",
            "Epoch 282/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2866 - accuracy: 0.9144 - val_loss: 2.4129 - val_accuracy: 0.6364\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 1.18448\n",
            "Epoch 283/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2886 - accuracy: 0.9136 - val_loss: 2.4055 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.18448\n",
            "Epoch 284/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2956 - accuracy: 0.9122 - val_loss: 2.3990 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.18448\n",
            "Epoch 285/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2869 - accuracy: 0.9162 - val_loss: 2.4087 - val_accuracy: 0.6365\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.18448\n",
            "Epoch 286/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2883 - accuracy: 0.9149 - val_loss: 2.4022 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.18448\n",
            "Epoch 287/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2865 - accuracy: 0.9108 - val_loss: 2.3917 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.18448\n",
            "Epoch 288/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2901 - accuracy: 0.9114 - val_loss: 2.3946 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.18448\n",
            "Epoch 289/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2833 - accuracy: 0.9151 - val_loss: 2.3831 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.18448\n",
            "Epoch 290/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2887 - accuracy: 0.9155 - val_loss: 2.3901 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.18448\n",
            "Epoch 291/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2861 - accuracy: 0.9141 - val_loss: 2.3975 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.18448\n",
            "Epoch 292/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2758 - accuracy: 0.9146 - val_loss: 2.3988 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.18448\n",
            "Epoch 293/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2983 - accuracy: 0.9118 - val_loss: 2.3827 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.18448\n",
            "Epoch 294/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2778 - accuracy: 0.9136 - val_loss: 2.3963 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.18448\n",
            "Epoch 295/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2924 - accuracy: 0.9128 - val_loss: 2.3853 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.18448\n",
            "Epoch 296/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2913 - accuracy: 0.9125 - val_loss: 2.3886 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.18448\n",
            "Epoch 297/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2861 - accuracy: 0.9152 - val_loss: 2.4000 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.18448\n",
            "Epoch 298/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2898 - accuracy: 0.9103 - val_loss: 2.3977 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.18448\n",
            "Epoch 299/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2792 - accuracy: 0.9129 - val_loss: 2.4029 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.18448\n",
            "Epoch 300/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2832 - accuracy: 0.9158 - val_loss: 2.4077 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.18448\n",
            "Epoch 301/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2950 - accuracy: 0.9116 - val_loss: 2.3877 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 1.18448\n",
            "Epoch 302/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2893 - accuracy: 0.9145 - val_loss: 2.3953 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 1.18448\n",
            "Epoch 303/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2972 - accuracy: 0.9120 - val_loss: 2.3870 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 1.18448\n",
            "Epoch 304/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2767 - accuracy: 0.9172 - val_loss: 2.4190 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 1.18448\n",
            "Epoch 305/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2835 - accuracy: 0.9139 - val_loss: 2.4002 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 1.18448\n",
            "Epoch 306/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2710 - accuracy: 0.9202 - val_loss: 2.4096 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 1.18448\n",
            "Epoch 307/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2951 - accuracy: 0.9112 - val_loss: 2.4081 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 1.18448\n",
            "Epoch 308/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2924 - accuracy: 0.9117 - val_loss: 2.4012 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 1.18448\n",
            "Epoch 309/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2826 - accuracy: 0.9155 - val_loss: 2.3990 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 1.18448\n",
            "Epoch 310/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2964 - accuracy: 0.9110 - val_loss: 2.3969 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 1.18448\n",
            "Epoch 311/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2928 - accuracy: 0.9120 - val_loss: 2.3999 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 1.18448\n",
            "Epoch 312/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2956 - accuracy: 0.9081 - val_loss: 2.3999 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 1.18448\n",
            "Epoch 313/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3023 - accuracy: 0.9095 - val_loss: 2.4010 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 1.18448\n",
            "Epoch 314/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2906 - accuracy: 0.9111 - val_loss: 2.3927 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 1.18448\n",
            "Epoch 315/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2879 - accuracy: 0.9121 - val_loss: 2.3984 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 1.18448\n",
            "Epoch 316/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2873 - accuracy: 0.9164 - val_loss: 2.4130 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 1.18448\n",
            "Epoch 317/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2844 - accuracy: 0.9146 - val_loss: 2.4104 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 1.18448\n",
            "Epoch 318/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2933 - accuracy: 0.9106 - val_loss: 2.3955 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 1.18448\n",
            "Epoch 319/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2927 - accuracy: 0.9122 - val_loss: 2.3922 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 1.18448\n",
            "Epoch 320/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2950 - accuracy: 0.9096 - val_loss: 2.3947 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 1.18448\n",
            "Epoch 321/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2846 - accuracy: 0.9141 - val_loss: 2.3948 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 1.18448\n",
            "Epoch 322/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3018 - accuracy: 0.9065 - val_loss: 2.3867 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 1.18448\n",
            "Epoch 323/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3074 - accuracy: 0.9072 - val_loss: 2.3901 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 1.18448\n",
            "Epoch 324/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3061 - accuracy: 0.9075 - val_loss: 2.4036 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 1.18448\n",
            "Epoch 325/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2925 - accuracy: 0.9116 - val_loss: 2.3890 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 1.18448\n",
            "Epoch 326/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2830 - accuracy: 0.9155 - val_loss: 2.3913 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 1.18448\n",
            "Epoch 327/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2915 - accuracy: 0.9114 - val_loss: 2.4162 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 1.18448\n",
            "Epoch 328/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2878 - accuracy: 0.9137 - val_loss: 2.3900 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 1.18448\n",
            "Epoch 329/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2846 - accuracy: 0.9172 - val_loss: 2.4065 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 1.18448\n",
            "Epoch 330/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2897 - accuracy: 0.9109 - val_loss: 2.4133 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 1.18448\n",
            "Epoch 331/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3016 - accuracy: 0.9090 - val_loss: 2.4210 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 1.18448\n",
            "Epoch 332/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.2842 - accuracy: 0.9147 - val_loss: 2.4057 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 1.18448\n",
            "Epoch 333/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2784 - accuracy: 0.9179 - val_loss: 2.3895 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 1.18448\n",
            "Epoch 334/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2920 - accuracy: 0.9115 - val_loss: 2.3806 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 1.18448\n",
            "Epoch 335/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2889 - accuracy: 0.9126 - val_loss: 2.3933 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 1.18448\n",
            "Epoch 336/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2963 - accuracy: 0.9113 - val_loss: 2.3955 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 1.18448\n",
            "Epoch 337/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2799 - accuracy: 0.9163 - val_loss: 2.3969 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 1.18448\n",
            "Epoch 338/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2821 - accuracy: 0.9141 - val_loss: 2.3998 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 1.18448\n",
            "Epoch 339/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2854 - accuracy: 0.9154 - val_loss: 2.3954 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 1.18448\n",
            "Epoch 340/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2858 - accuracy: 0.9153 - val_loss: 2.4008 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 1.18448\n",
            "Epoch 341/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2852 - accuracy: 0.9140 - val_loss: 2.4048 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 1.18448\n",
            "Epoch 342/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2944 - accuracy: 0.9118 - val_loss: 2.3999 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 1.18448\n",
            "Epoch 343/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2902 - accuracy: 0.9093 - val_loss: 2.3973 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 1.18448\n",
            "Epoch 344/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2875 - accuracy: 0.9149 - val_loss: 2.4105 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 1.18448\n",
            "Epoch 345/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2863 - accuracy: 0.9130 - val_loss: 2.4058 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 1.18448\n",
            "Epoch 346/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2984 - accuracy: 0.9093 - val_loss: 2.4069 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 1.18448\n",
            "Epoch 347/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2875 - accuracy: 0.9128 - val_loss: 2.3914 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 1.18448\n",
            "Epoch 348/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2904 - accuracy: 0.9082 - val_loss: 2.4048 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 1.18448\n",
            "Epoch 349/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2945 - accuracy: 0.9139 - val_loss: 2.3943 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 1.18448\n",
            "Epoch 350/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2862 - accuracy: 0.9123 - val_loss: 2.3932 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 1.18448\n",
            "Epoch 351/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2914 - accuracy: 0.9101 - val_loss: 2.3969 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 1.18448\n",
            "Epoch 352/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2864 - accuracy: 0.9143 - val_loss: 2.3969 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 1.18448\n",
            "Epoch 353/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2950 - accuracy: 0.9133 - val_loss: 2.3965 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 1.18448\n",
            "Epoch 354/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2844 - accuracy: 0.9131 - val_loss: 2.4014 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 1.18448\n",
            "Epoch 355/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2984 - accuracy: 0.9112 - val_loss: 2.4034 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 1.18448\n",
            "Epoch 356/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2860 - accuracy: 0.9136 - val_loss: 2.3874 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 1.18448\n",
            "Epoch 357/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2972 - accuracy: 0.9138 - val_loss: 2.3832 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 1.18448\n",
            "Epoch 358/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2950 - accuracy: 0.9113 - val_loss: 2.4112 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 1.18448\n",
            "Epoch 359/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.2833 - accuracy: 0.9144 - val_loss: 2.3953 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 1.18448\n",
            "Epoch 360/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2816 - accuracy: 0.9134 - val_loss: 2.3896 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 1.18448\n",
            "Epoch 361/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2864 - accuracy: 0.9149 - val_loss: 2.4028 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 1.18448\n",
            "Epoch 362/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2870 - accuracy: 0.9144 - val_loss: 2.3924 - val_accuracy: 0.6387\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 1.18448\n",
            "Epoch 363/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2844 - accuracy: 0.9125 - val_loss: 2.3817 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 1.18448\n",
            "Epoch 364/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2823 - accuracy: 0.9146 - val_loss: 2.4044 - val_accuracy: 0.6364\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 1.18448\n",
            "Epoch 365/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2834 - accuracy: 0.9132 - val_loss: 2.3944 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 1.18448\n",
            "Epoch 366/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2879 - accuracy: 0.9133 - val_loss: 2.4028 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 1.18448\n",
            "Epoch 367/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2924 - accuracy: 0.9125 - val_loss: 2.4090 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 1.18448\n",
            "Epoch 368/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3023 - accuracy: 0.9086 - val_loss: 2.3915 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 1.18448\n",
            "Epoch 369/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2830 - accuracy: 0.9129 - val_loss: 2.4076 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 1.18448\n",
            "Epoch 370/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2994 - accuracy: 0.9089 - val_loss: 2.4162 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 1.18448\n",
            "Epoch 371/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2810 - accuracy: 0.9137 - val_loss: 2.4086 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 1.18448\n",
            "Epoch 372/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2881 - accuracy: 0.9144 - val_loss: 2.4036 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 1.18448\n",
            "Epoch 373/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2946 - accuracy: 0.9121 - val_loss: 2.3956 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 1.18448\n",
            "Epoch 374/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2803 - accuracy: 0.9157 - val_loss: 2.3967 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 1.18448\n",
            "Epoch 375/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2871 - accuracy: 0.9152 - val_loss: 2.3930 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 1.18448\n",
            "Epoch 376/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2909 - accuracy: 0.9125 - val_loss: 2.3956 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 1.18448\n",
            "Epoch 377/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2892 - accuracy: 0.9128 - val_loss: 2.3954 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 1.18448\n",
            "Epoch 378/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2893 - accuracy: 0.9136 - val_loss: 2.3956 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 1.18448\n",
            "Epoch 379/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2930 - accuracy: 0.9150 - val_loss: 2.4132 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 1.18448\n",
            "Epoch 380/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2894 - accuracy: 0.9130 - val_loss: 2.3758 - val_accuracy: 0.6365\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 1.18448\n",
            "Epoch 381/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2952 - accuracy: 0.9106 - val_loss: 2.4116 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 1.18448\n",
            "Epoch 382/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2845 - accuracy: 0.9128 - val_loss: 2.3944 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 1.18448\n",
            "Epoch 383/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2877 - accuracy: 0.9142 - val_loss: 2.3902 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 1.18448\n",
            "Epoch 384/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2934 - accuracy: 0.9112 - val_loss: 2.3949 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 1.18448\n",
            "Epoch 385/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2899 - accuracy: 0.9124 - val_loss: 2.4041 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 1.18448\n",
            "Epoch 386/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2918 - accuracy: 0.9128 - val_loss: 2.3888 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 1.18448\n",
            "Epoch 387/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2855 - accuracy: 0.9139 - val_loss: 2.4011 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 1.18448\n",
            "Epoch 388/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2930 - accuracy: 0.9112 - val_loss: 2.3952 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 1.18448\n",
            "Epoch 389/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2928 - accuracy: 0.9141 - val_loss: 2.3962 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 1.18448\n",
            "Epoch 390/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2893 - accuracy: 0.9123 - val_loss: 2.4055 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 1.18448\n",
            "Epoch 391/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2953 - accuracy: 0.9141 - val_loss: 2.3984 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 1.18448\n",
            "Epoch 392/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2827 - accuracy: 0.9126 - val_loss: 2.4050 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 1.18448\n",
            "Epoch 393/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2918 - accuracy: 0.9139 - val_loss: 2.4020 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 1.18448\n",
            "Epoch 394/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2901 - accuracy: 0.9112 - val_loss: 2.4098 - val_accuracy: 0.6359\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 1.18448\n",
            "Epoch 395/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2848 - accuracy: 0.9160 - val_loss: 2.4149 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 1.18448\n",
            "Epoch 396/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2876 - accuracy: 0.9128 - val_loss: 2.3969 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 1.18448\n",
            "Epoch 397/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2848 - accuracy: 0.9132 - val_loss: 2.3947 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 1.18448\n",
            "Epoch 398/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2937 - accuracy: 0.9117 - val_loss: 2.3986 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 1.18448\n",
            "Epoch 399/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2800 - accuracy: 0.9145 - val_loss: 2.3961 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 1.18448\n",
            "Epoch 400/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2947 - accuracy: 0.9120 - val_loss: 2.4068 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 1.18448\n",
            "Epoch 401/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2889 - accuracy: 0.9121 - val_loss: 2.3974 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 1.18448\n",
            "Epoch 402/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2882 - accuracy: 0.9121 - val_loss: 2.3954 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 1.18448\n",
            "Epoch 403/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2834 - accuracy: 0.9156 - val_loss: 2.3990 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 1.18448\n",
            "Epoch 404/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2913 - accuracy: 0.9128 - val_loss: 2.4082 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 1.18448\n",
            "Epoch 405/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2843 - accuracy: 0.9144 - val_loss: 2.3990 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 1.18448\n",
            "Epoch 406/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2945 - accuracy: 0.9122 - val_loss: 2.4094 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 1.18448\n",
            "Epoch 407/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2890 - accuracy: 0.9120 - val_loss: 2.4122 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 1.18448\n",
            "Epoch 408/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2843 - accuracy: 0.9103 - val_loss: 2.3980 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 1.18448\n",
            "Epoch 409/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2862 - accuracy: 0.9125 - val_loss: 2.4022 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 1.18448\n",
            "Epoch 410/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2923 - accuracy: 0.9114 - val_loss: 2.3840 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 1.18448\n",
            "Epoch 411/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2890 - accuracy: 0.9173 - val_loss: 2.4062 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 1.18448\n",
            "Epoch 412/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2745 - accuracy: 0.9164 - val_loss: 2.4101 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 1.18448\n",
            "Epoch 413/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2978 - accuracy: 0.9111 - val_loss: 2.4040 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 1.18448\n",
            "Epoch 414/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2873 - accuracy: 0.9126 - val_loss: 2.4017 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 1.18448\n",
            "Epoch 415/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2926 - accuracy: 0.9148 - val_loss: 2.4128 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 1.18448\n",
            "Epoch 416/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2835 - accuracy: 0.9120 - val_loss: 2.4002 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 1.18448\n",
            "Epoch 417/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2910 - accuracy: 0.9147 - val_loss: 2.3897 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 1.18448\n",
            "Epoch 418/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2911 - accuracy: 0.9157 - val_loss: 2.4071 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 1.18448\n",
            "Epoch 419/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2947 - accuracy: 0.9115 - val_loss: 2.3955 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 1.18448\n",
            "Epoch 420/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2831 - accuracy: 0.9163 - val_loss: 2.3901 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 1.18448\n",
            "Epoch 421/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2801 - accuracy: 0.9135 - val_loss: 2.3980 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 1.18448\n",
            "Epoch 422/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2904 - accuracy: 0.9134 - val_loss: 2.3864 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 1.18448\n",
            "Epoch 423/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2917 - accuracy: 0.9097 - val_loss: 2.3966 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 1.18448\n",
            "Epoch 424/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2942 - accuracy: 0.9125 - val_loss: 2.4092 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 1.18448\n",
            "Epoch 425/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2834 - accuracy: 0.9153 - val_loss: 2.3979 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 1.18448\n",
            "Epoch 426/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2939 - accuracy: 0.9144 - val_loss: 2.3904 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 1.18448\n",
            "Epoch 427/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2926 - accuracy: 0.9105 - val_loss: 2.4016 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 1.18448\n",
            "Epoch 428/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2792 - accuracy: 0.9154 - val_loss: 2.4097 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 1.18448\n",
            "Epoch 429/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2933 - accuracy: 0.9122 - val_loss: 2.4027 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 1.18448\n",
            "Epoch 430/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2849 - accuracy: 0.9135 - val_loss: 2.4004 - val_accuracy: 0.6365\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 1.18448\n",
            "Epoch 431/500\n",
            "404/404 [==============================] - 25s 63ms/step - loss: 0.2891 - accuracy: 0.9130 - val_loss: 2.3903 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 1.18448\n",
            "Epoch 432/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2974 - accuracy: 0.9101 - val_loss: 2.3964 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 1.18448\n",
            "Epoch 433/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2799 - accuracy: 0.9154 - val_loss: 2.4126 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 1.18448\n",
            "Epoch 434/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2891 - accuracy: 0.9114 - val_loss: 2.3987 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 1.18448\n",
            "Epoch 435/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2825 - accuracy: 0.9130 - val_loss: 2.3917 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 1.18448\n",
            "Epoch 436/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2890 - accuracy: 0.9113 - val_loss: 2.4068 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 1.18448\n",
            "Epoch 437/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2879 - accuracy: 0.9121 - val_loss: 2.3973 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 1.18448\n",
            "Epoch 438/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2840 - accuracy: 0.9120 - val_loss: 2.3868 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 1.18448\n",
            "Epoch 439/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2868 - accuracy: 0.9182 - val_loss: 2.4007 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 1.18448\n",
            "Epoch 440/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2754 - accuracy: 0.9166 - val_loss: 2.4068 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 1.18448\n",
            "Epoch 441/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2945 - accuracy: 0.9132 - val_loss: 2.3944 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 1.18448\n",
            "Epoch 442/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2900 - accuracy: 0.9130 - val_loss: 2.4084 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 1.18448\n",
            "Epoch 443/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2979 - accuracy: 0.9092 - val_loss: 2.3976 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 1.18448\n",
            "Epoch 444/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2912 - accuracy: 0.9137 - val_loss: 2.4030 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 1.18448\n",
            "Epoch 445/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2747 - accuracy: 0.9163 - val_loss: 2.4009 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 1.18448\n",
            "Epoch 446/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2816 - accuracy: 0.9152 - val_loss: 2.4005 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 1.18448\n",
            "Epoch 447/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2937 - accuracy: 0.9127 - val_loss: 2.4114 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 1.18448\n",
            "Epoch 448/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2892 - accuracy: 0.9136 - val_loss: 2.3977 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 1.18448\n",
            "Epoch 449/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2902 - accuracy: 0.9132 - val_loss: 2.4019 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 1.18448\n",
            "Epoch 450/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2887 - accuracy: 0.9146 - val_loss: 2.4094 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 1.18448\n",
            "Epoch 451/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2850 - accuracy: 0.9127 - val_loss: 2.3898 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 1.18448\n",
            "Epoch 452/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2930 - accuracy: 0.9144 - val_loss: 2.4053 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 1.18448\n",
            "Epoch 453/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2904 - accuracy: 0.9147 - val_loss: 2.4017 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 1.18448\n",
            "Epoch 454/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2929 - accuracy: 0.9122 - val_loss: 2.3976 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 1.18448\n",
            "Epoch 455/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2876 - accuracy: 0.9148 - val_loss: 2.4014 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 1.18448\n",
            "Epoch 456/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2898 - accuracy: 0.9150 - val_loss: 2.3896 - val_accuracy: 0.6365\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 1.18448\n",
            "Epoch 457/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2910 - accuracy: 0.9126 - val_loss: 2.4020 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 1.18448\n",
            "Epoch 458/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2832 - accuracy: 0.9162 - val_loss: 2.3942 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 1.18448\n",
            "Epoch 459/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2853 - accuracy: 0.9127 - val_loss: 2.4061 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 1.18448\n",
            "Epoch 460/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2923 - accuracy: 0.9108 - val_loss: 2.4108 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 1.18448\n",
            "Epoch 461/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3050 - accuracy: 0.9089 - val_loss: 2.4039 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 1.18448\n",
            "Epoch 462/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2948 - accuracy: 0.9112 - val_loss: 2.3920 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 1.18448\n",
            "Epoch 463/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2821 - accuracy: 0.9148 - val_loss: 2.3895 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 1.18448\n",
            "Epoch 464/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2969 - accuracy: 0.9090 - val_loss: 2.3854 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 1.18448\n",
            "Epoch 465/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2834 - accuracy: 0.9140 - val_loss: 2.3993 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 1.18448\n",
            "Epoch 466/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2893 - accuracy: 0.9133 - val_loss: 2.3972 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 1.18448\n",
            "Epoch 467/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2868 - accuracy: 0.9128 - val_loss: 2.3990 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 1.18448\n",
            "Epoch 468/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2872 - accuracy: 0.9111 - val_loss: 2.4081 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 1.18448\n",
            "Epoch 469/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2955 - accuracy: 0.9093 - val_loss: 2.3972 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 1.18448\n",
            "Epoch 470/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2891 - accuracy: 0.9116 - val_loss: 2.3863 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 1.18448\n",
            "Epoch 471/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2872 - accuracy: 0.9113 - val_loss: 2.3990 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 1.18448\n",
            "Epoch 472/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2842 - accuracy: 0.9128 - val_loss: 2.4095 - val_accuracy: 0.6368\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 1.18448\n",
            "Epoch 473/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2836 - accuracy: 0.9172 - val_loss: 2.4016 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 1.18448\n",
            "Epoch 474/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2969 - accuracy: 0.9111 - val_loss: 2.3975 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 1.18448\n",
            "Epoch 475/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2855 - accuracy: 0.9152 - val_loss: 2.3844 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 1.18448\n",
            "Epoch 476/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2824 - accuracy: 0.9167 - val_loss: 2.3872 - val_accuracy: 0.6382\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 1.18448\n",
            "Epoch 477/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2946 - accuracy: 0.9109 - val_loss: 2.3841 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 1.18448\n",
            "Epoch 478/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2841 - accuracy: 0.9148 - val_loss: 2.3971 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 1.18448\n",
            "Epoch 479/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2899 - accuracy: 0.9132 - val_loss: 2.4127 - val_accuracy: 0.6384\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 1.18448\n",
            "Epoch 480/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2999 - accuracy: 0.9098 - val_loss: 2.3914 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 1.18448\n",
            "Epoch 481/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2907 - accuracy: 0.9120 - val_loss: 2.3824 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 1.18448\n",
            "Epoch 482/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2890 - accuracy: 0.9136 - val_loss: 2.4046 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 1.18448\n",
            "Epoch 483/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2906 - accuracy: 0.9120 - val_loss: 2.4044 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 1.18448\n",
            "Epoch 484/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2890 - accuracy: 0.9141 - val_loss: 2.3885 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 1.18448\n",
            "Epoch 485/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2962 - accuracy: 0.9093 - val_loss: 2.4011 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 1.18448\n",
            "Epoch 486/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2810 - accuracy: 0.9153 - val_loss: 2.4087 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 1.18448\n",
            "Epoch 487/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2846 - accuracy: 0.9132 - val_loss: 2.3962 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 1.18448\n",
            "Epoch 488/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.3014 - accuracy: 0.9099 - val_loss: 2.3998 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 1.18448\n",
            "Epoch 489/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2907 - accuracy: 0.9138 - val_loss: 2.4090 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 1.18448\n",
            "Epoch 490/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2847 - accuracy: 0.9161 - val_loss: 2.4040 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 1.18448\n",
            "Epoch 491/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2812 - accuracy: 0.9160 - val_loss: 2.3968 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 1.18448\n",
            "Epoch 492/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2886 - accuracy: 0.9100 - val_loss: 2.4035 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 1.18448\n",
            "Epoch 493/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2930 - accuracy: 0.9134 - val_loss: 2.3960 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 1.18448\n",
            "Epoch 494/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2804 - accuracy: 0.9145 - val_loss: 2.3889 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 1.18448\n",
            "Epoch 495/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2918 - accuracy: 0.9107 - val_loss: 2.4020 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 1.18448\n",
            "Epoch 496/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2834 - accuracy: 0.9138 - val_loss: 2.3975 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 1.18448\n",
            "Epoch 497/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2844 - accuracy: 0.9129 - val_loss: 2.3976 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 1.18448\n",
            "Epoch 498/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2862 - accuracy: 0.9135 - val_loss: 2.3870 - val_accuracy: 0.6376\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 1.18448\n",
            "Epoch 499/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2888 - accuracy: 0.9118 - val_loss: 2.4205 - val_accuracy: 0.6385\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 1.18448\n",
            "Epoch 500/500\n",
            "404/404 [==============================] - 25s 62ms/step - loss: 0.2867 - accuracy: 0.9145 - val_loss: 2.4039 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 1.18448\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD8ppILhFUrz"
      },
      "source": [
        "## 6. Test the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "OzhQw9NOWjl3",
        "outputId": "2801b700-a48b-43fc-b6a4-fc5521c929a7"
      },
      "source": [
        "predicted_test_labels = np.argmax(model.predict(test_data), axis=1)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "print (\"Accuracy score = \", accuracy_score(test_labels, predicted_test_labels))\n",
        "#print(\"PTL: \", model.predict(test_data))\n",
        "#print(\"TL: \", test_labels)\n",
        "plt.plot(model.predict(test_data))\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(test_data)\n",
        "y_test = test_labels\n",
        "ranks = 7\n",
        "\n",
        "def calculate_cmc(y_pred, y_test, num_ranks=10):\n",
        "\n",
        "    ranks = num_ranks\n",
        "    rank_scores = np.zeros(ranks)\n",
        "\n",
        "    for el_pred, el_true in zip(y_pred, y_test):   \n",
        "        for r in range(ranks):\n",
        "            r_max_val_ind = np.argsort(el_pred)[-(r+1):][0]\n",
        "            r_max_val = el_pred[r_max_val_ind]\n",
        "            correct_ranks = ((el_pred >= r_max_val) * 1)\n",
        "\n",
        "            yind = np.argmax(el_true, axis=-1)\n",
        "            if correct_ranks[yind] == 1:\n",
        "                rank_scores[r] += 1\n",
        "\n",
        "\n",
        "    rank_scores = rank_scores / y_pred.shape[0]\n",
        "    return rank_scores\n",
        "\n",
        "scores = calculate_cmc(y_pred, y_test, num_ranks=ranks)\n",
        "\n",
        "plt.plot(np.arange(1,ranks+1), scores*100)\n",
        "plt.title(\"CMC Curve\")\n",
        "plt.xlabel(\"Rank\")\n",
        "plt.xticks(np.arange(1,ranks+1))\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.show()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score =  0.636667595430482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGsta6kKHr82"
      },
      "source": [
        "## 7. Save the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiMswpPsr4KK",
        "outputId": "ca8792b7-f85a-4476-cdc6-846426a3c45b"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/gdrive/My Drive/Colab Notebooks/Model500/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/Model500/model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}